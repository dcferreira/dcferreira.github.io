[{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.   By this point we have everything on the backend already setup, we just need a nice way to interact with it. In this post we’ll make a very simple website which will call our backend and display the results using a bit of JavaScript.\nBasic Functionality The first thing to do is to get a rough sketch of what we need to display. We’ll make a index.html file in the root of our project:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;AI Web App\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Search\u0026lt;/h1\u0026gt; \u0026lt;div\u0026gt; Search query: \u0026lt;label for=\u0026#34;formQuery\u0026#34;\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;input id=\u0026#34;formQuery\u0026#34; type=\u0026#34;text\u0026#34; /\u0026gt;\u0026lt;br /\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;Search!\u0026#34; onclick=\u0026#34;search()\u0026#34; /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;table id=\u0026#34;results\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;Score\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Text\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;Title\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This is just a simple text field where you will write the query, a button to execute it, and a table in the bottom where the results will appear. When the button is pressed, the JavaScript engine calls the search function, which we still need to write.\nLet’s add the search function to the \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt; section of the page which takes the input from the text field, submits a request to our backend, and populates the table with the results:\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; function processResult(result) { return `\u0026lt;td\u0026gt;${result.score}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;${result.text}\u0026lt;/td\u0026gt;\u0026lt;td\u0026gt;${result.title}\u0026lt;/td\u0026gt;`; } function search() { const query = document.getElementById(\u0026#34;formQuery\u0026#34;); if (query) { const uri = encodeURI(`http://127.0.0.1:8080/search?query=${query}`); fetch(uri) .then((response) =\u0026gt; response.json()) .then((data) =\u0026gt; { console.log(data); document.getElementById(\u0026#34;results\u0026#34;).innerHTML = \u0026#34;\u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Score\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Text\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Title\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026#34; + data.map(processResult).join(\u0026#34;\u0026lt;/tr\u0026gt;\u0026lt;tr\u0026gt;\u0026#34;) + \u0026#34;\u0026lt;/tr\u0026gt;\u0026#34;; }) .catch((error) =\u0026gt; { console.error(error); }); } } \u0026lt;/script\u0026gt; This code assumes our backend is deployed locally (at http://127.0.0.1:8080). Later on this URL will need to be changed to the URL of our backend, wherever it is deployed (see Part 8).\nRunning To test this, open up 2 terminal windows. In one, run the backend\nhatch run serve and in the other one, run the frontend\npython -m http.server You can now visit http://127.0.0.1:8000 and will see the page   You can try writing a query and submitting it. Your browser will display an error: CORS doesn’t allow the browser to fetch content from http://127.0.0.1:8080 (the backend) while the user is visiting http://127.0.0.1:8000 (the frontend). We need to set some headers in our backend to allow this. In app.py add the CORS middleware, and configure it take the CORS origin from an environment variable CORS_ORIGIN. Add this code to the file (after the app variable is defined):\nfrom starlette.middleware.cors import CORSMiddleware def get_cors_origin() -\u0026gt; str: cors_origin = os.getenv(\u0026#34;CORS_ORIGIN\u0026#34;) if cors_origin is None: raise ValueError( \u0026#34;`CORS_ORIGIN` environment variable not defined! \u0026#34; \u0026#34;Please set this to the domain from which requests \u0026#34; \u0026#34;to the backend will be made.\u0026#34; ) return cors_origin origins = [ get_cors_origin(), ] app.add_middleware( CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;], ) Now stop the backend and run it again with\nCORS_ORIGIN=http://127.0.0.1:8000 hatch run serve and querying now works!   [Optional] Styling The table isn’t so easy to read. An easy fix is to add a style to it. We can copy the one from w3schools; just add this to the head section of the HTML:\n\u0026lt;style\u0026gt; table { font-family: arial, sans-serif; border-collapse: collapse; width: 100%; } td, th { border: 1px solid #dddddd; text-align: left; padding: 8px; } tr:nth-child(even) { background-color: #dddddd; } \u0026lt;/style\u0026gt; And now our table looks a lot better :)   Deploy You can now deploy this version of the backend to your cloud provider (see Part 8), and deploy the frontend somewhere that’s publicly accessible. There’s plenty of services where you can deploy a static frontend such as this. Some of the most popular ones are GitHub Pages, Netlify. You can also use a cloud blob storage service such as Google Cloud Storage or AWS S3, and point your own domain to the blob with the HTML.\nRemember to update your backend deployment with the new code above (introducing the CORS middleware), and to setup the CORS_ORIGIN to the URL of the frontend. Also don’t forget to update the URL of the backend in the HTML file above.\nAnd that’s all, we finally have a fully functioning AI web app! :)\n If you’ve been following these instructions, your code should look like this: https://github.com/dcferreira/ai-web-app/tree/656ebd505131d7f96e5366e1a3f0e5c76a1007c0   ","date":1678234140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678234140,"objectID":"b4b8494886ffbdf4b36f265bda9d0726","permalink":"https://dcferreira.com/post/2023-03-09-ai-web-app/","publishdate":"2023-03-08T00:09:00Z","relpermalink":"/post/2023-03-09-ai-web-app/","section":"post","summary":"How to make and deploy a simple frontend for an AI web app that interacts with a serverless backend.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 9)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.\nThis post uses Google’s Cloud Run. Alternatives would be AWS’s Lambda, Azure Functions.\n  It’s finally time to deploy our web app so that everyone can reach it. We’ll deploy our app in a serverless fashion. This means that every time someone makes a request, a new container will be started and run only for the needed time to respond to it. By doing it this way, we avoid wasting money on compute when no one is making requests to our server. Furthermore, scaling happens automatically: if suddenly our web app gets very popular, our cloud provider automatically starts as many containers as needed.\nIn this post we’ll do everything with the gcloud CLI, but it can all be done with the graphical interface of the Google Cloud Console website.\nSetup The first step is to install the gcloud CLI and run gcloud init to configure your account.\nThen we need the following setup steps:\n  Create a new project\ngcloud projects create my-example-webapp-23867 --name=\u0026#34;AI Web App\u0026#34; Note that the project ID (my-example-webapp-23867) needs to be unique. So this exact command won’t work for you, you will need to create your own project ID.\n  Configure gcloud to use your project:\ngcloud config set project my-example-webapp-23867   Activate the Cloud Run API\ngcloud services enable run.googleapis.com   Activate the Artifact Registry API, which we will use to upload our docker images.\ngcloud services enable artifactregistry.googleapis.com Note that this step requires that billing is enabled in the project. This step is not strictly required: you could upload the docker images somewhere else, such as docker hub.\nTo activate billing for the project, first set up a billing account on the Google Cloud Console. Using the CLI, you can list the existing billing accounts:\ngcloud billing accounts list You can then link the project to a specific billing account\ngcloud billing projects link my-example-webapp-23867 --billing-account 0X0X0X-0X0X0X-0X0X0X (don’t forget to replace the billing account ID by your own)\n  Create a new artifacts repository\ngcloud artifacts repositories create ai-web-app-artifacts --repository-format=docker --location=us-central1 --description=\u0026#34;Docker artifacts\u0026#34; Choose the location that is closer to you and your users.\nSetup your docker client to authenticate to this repo with\ngcloud auth configure-docker us-central1-docker.pkg.dev If you chose something else than us-central1 for the location in the last step, be sure to reflect that in this auth command. For example, if your region is europe-west1, you would instead run\ngcloud auth configure-docker europe-west1-docker.pkg.dev   Deploy By this point, we’ve done all the necessary setup to deploy our app. Now all that’s left is to upload our docker image and start our serverless function.\nWe should add a helper script in pyproject.toml to push our image to Google Cloud:\n[tool.hatch.envs.default.scripts] push = [ \u0026#34;docker tag ai-web-app:latest us-central1-docker.pkg.dev/my-example-webapp-23867/ai-web-app-artifacts/ai-web-app:latest\u0026#34;, \u0026#34;docker push us-central1-docker.pkg.dev/my-example-webapp-23867/ai-web-app-artifacts/ai-web-app:latest\u0026#34; ] We can then build and push our docker image\nhatch run build \u0026amp;\u0026amp; hatch run push And finally we can deploy our serverless function (when asked, choose unauthenticated access)\ngcloud run deploy ai-web-app --image=\u0026#34;us-central1-docker.pkg.dev/my-example-webapp-23867/ai-web-app-artifacts/ai-web-app:latest\u0026#34; --region=us-central1 --memory=2Gi Note that the --memory=2Gi option is important for this specific case, as the all-MiniLM-L6-v2 model (see Part 1) requires almost all that memory to run.\nThe output of this deploy command will give you a Service URL for your app, which you should use to access it.\nThe app should now be up (if it’s not, have a look at the Cloud Run logs for errors), and you can reach it with\ncurl -X GET \u0026#34;https://\u0026lt;SERVICE-URL\u0026gt;/search?query=risk%20factors\u0026#34;  About keeping your budget low:\n If you setup your Cloud Run function without authentication, it’s a good idea to set budget limits and avoid unpleasant billing surprises. If you’re pushing many images to the Artifact Registry, the costs can also quickly add up. You should regularly delete old images, or create a cleanup policy.    To continue this tutorial, go to Part 9.\n","date":1678234080,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678234080,"objectID":"ab684eceb3ecc9662d3785b82f7cfc16","permalink":"https://dcferreira.com/post/2023-03-08-ai-web-app/","publishdate":"2023-03-08T00:08:00Z","relpermalink":"/post/2023-03-08-ai-web-app/","section":"post","summary":"How to deploy a serverless AI app to Google Cloud almost for free using Cloud Run and the Artifact Registry.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 8)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.\nThis post uses GitHub Actions. Possible alternatives are: GitLab Pipelines, Jenkins, pypyr, and many others.\n  Now that we have a first version of our app working, it’s time to setup a CI pipeline. In this post, we’ll make a very simple pipeline which runs linting and unit/integration tests. We’ll use GitHub Actions for this post.\nSetup CI Pipeline The first thing we need to do is create a file .github/workflows/backend.yaml on the root of our repository:\nname:Backend CIon:pushjobs:run:name:Run on Python ${{ matrix.python-version }} (${{ matrix.os }})runs-on:${{ matrix.os }}strategy:matrix:python-version:[\u0026#34;3.10\u0026#34;]# can use other python versionsos:[\u0026#34;ubuntu-latest\u0026#34;]# can test in other OSsteps:- uses:actions/checkout@v3- name:Check for CRLF endingsuses:erclu/check-crlf@v1- name:Set up Python ${{ matrix.python-version }}uses:actions/setup-python@v4with:python-version:${{ matrix.python-version }}- name:Install hatchrun:python -m pip install -U pip hatch- name:Lintrun:hatch run lint- name:Testrun:hatch run covThe file above will run a job that clones our repo, checks if all files have LF line endings (if you’re using Windows to develop, that can be a common issue), installs Python and hatch, and runs our linter and unit tests.\nUsing the hatch scripts (see Part 3 for details) greatly simplifies the pipeline. With the scripts, it’s easy to make sure that we run the same command locally as on the CI pipeline, since all the parameters are in the config file.\nWe also want to run the integration tests. For that, we will need to download our database (or, if it’s too big, a subset of our real database). This download will take some time, and the integration tests themselves are also heavy. Therefore, we don’t want to run this job for every commit. In this case, we’ll limit the integration tests job to run only on commits in Pull Requests, or in the main branch.\nThis is our job configuration (just append to .github/workflows/backend.yaml):\nintegration:runs-on:ubuntu-latest# only run on pull requests and main branchif:${{ github.event_name == \u0026#39;pull_request\u0026#39; || github.ref == \u0026#39;refs/heads/main\u0026#39; }}steps:- uses:actions/checkout@v3- name:Set up Python 3.10uses:actions/setup-python@v4with:python-version:\u0026#34;3.10\u0026#34;- name:Download databaserun:|wget https://github.com/neuml/txtai/releases/download/v1.1.0/tests.gz mv tests.gz articles.sqlite.gz gunzip articles.sqlite- name:Install hatchrun:python -m pip install hatch- name:Run integration testsrun:hatch run integrationOn your first setup of the CI pipeline, it’s very common that something doesn’t behave as you expected, since you’re running the code on a different machine for the first time (even if the same code!). In my case, I had to make some small changes to my code to get it working.\nHaving this pipeline always running is a pretty good start, as it ensures that tests are run regularly and you’re notified if they fail. For next things to do with the pipelines, you can have a look at building/pushing docker images (such as the one we built in Part 6).\n If you’ve been following these instructions, your code should look like this: https://github.com/dcferreira/ai-web-app/tree/b91b29a4baf45969446068c1b2b53913b5f933ee   To continue this tutorial, go to Part 8.\n","date":1678234020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678234020,"objectID":"ae2ef289d43e1a419547acafc2dd4457","permalink":"https://dcferreira.com/post/2023-03-07-ai-web-app/","publishdate":"2023-03-08T00:07:00Z","relpermalink":"/post/2023-03-07-ai-web-app/","section":"post","summary":"How to develop a simple CI/CD pipeline for testing your code with GitHub Actions.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 7)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.   Now that your web app is working locally, it’s time to think about deploying it somewhere. The easiest way to do that is to make a Docker image, and send it to wherever you want to deploy it. Having a docker image guarantees that your environment is replicable wherever you take it.\nBuild a Docker Image In our app, we have a 2-stage process: first we build our Python package into a wheel file, and then we run the web server. Therefore, we can use Docker’s multi-stage build feature. Having this process split into 2 stages enables us to have a minimalist Python environment in the final image. That is, we install the dev environment in the first stage, but only the minimal needed dependencies in the final image.\nThis is our Dockerfile, which should be in the root of our project:\nFROMpython:3.10 AS builder# install hatchRUN pip install --no-cache-dir --upgrade hatchCOPY . /code# build python packageWORKDIR/codeRUN hatch build -t wheelFROMpython:3.10# copy wheel package from stage 1COPY --from=builder /code/dist /code/distRUN pip install --no-cache-dir --upgrade /code/dist/*# copy the serving code and our databaseCOPY app.py /codeCOPY articles.sqlite /code# run web serverWORKDIR/codeCMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;app:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8080\u0026#34;] If your project directory contains many files (or some big ones), building this image might take a long time. If that is the case, you should make a .dockerignore file to avoid having docker run through all your files. See more about this in Docker’s docs.   To simplify our workflow, we should also add a couple of scripts to our pyproject.toml, to build and run our docker image:\n[tool.hatch.envs.default.scripts] build = \u0026#34;docker buildx build . -t ai-web-app:latest\u0026#34; serve-docker = [\u0026#34;docker run -p 5000:8080 ai-web-app:latest\u0026#34;] You can then run\nhatch run build and the docker image will be built. Afterwards, you should run\nhatch run serve-docker which will serve the app in your local port 5000.\nYou can again test with curl, by running this command in a new terminal window:\ncurl -X GET \u0026#34;http://127.0.0.1:5000/search?query=symptoms%20of%20covid\u0026#34; This request should get the same result as the one in Part 5\n If you’ve been following these instructions, your code should look like this: https://github.com/dcferreira/ai-web-app/tree/1a9b73f17d99536801e6672d6161056d71fa44b4   [Optional] Optimize the Image In this case, the container startup is taking around 2 minutes in my machine. That could be fine if we’re deploying the image in our own premises, and we just start it up once and it will running. However, we want to deploy it to some serverless provider, which means the container will need to startup from scratch more or less for each request.\nTherefore, we should have a look at what we can do to minimize this startup time. In our app, every time the container starts, the database is being indexed. Another thing that takes time is downloading the all-MiniLM-L6-v2 model (see Part 2), which is also happening every time the container starts.\nThere is an easy fix in this case: since the model and the database will be the same for all containers we launch, we should just move these 2 steps to the build process. The build process will then take a bit more time, but the startup will be almost instant.\nIndeed this is what we did in this commit. After doing those optimizations, the startup is now almost instant.\nTo continue this tutorial, go to Part 7.\n","date":1678233960,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233960,"objectID":"e5c56e263fcf91f851c2311ea5982848","permalink":"https://dcferreira.com/post/2023-03-06-ai-web-app/","publishdate":"2023-03-08T00:06:00Z","relpermalink":"/post/2023-03-06-ai-web-app/","section":"post","summary":"How to use multi-stage building in Docker to containerize a hatch project with a web app.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 6)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.\nThis post uses FastAPI as the web framework and Uvicorn as the web server.\nSome alternatives to FastAPI are Flask, Starlette, and Django. Alternatives to Uvicorn include Gunicorn, Hypercorn, nginx.\n  By this point we have an AI project that we’re happy with, and want to make it easily available. The goal of this post is to write a simple backend service that calls the Python code from Part 4, and returns a result in some readable form, such as JSON.\nBackend For our case, we only want to expose the search function (see Part 4 for details).\nThe first thing to do is to add our dependencies to pyproject.toml. We want to add fastapi and uvicorn to our config file. FastAPI allows us to write Python code to define the API with type hints (what goes in and what comes out), as well as the Python logic to be executed (what happens in between). Uvicorn will allow us to actually serve our Python code as an HTTP service.\nLet’s start by making a new file app.py in the root of our project. In this file, we define a function search_endpoint which receives a query and calls our search function to query our database.\nfrom pathlib import Path from typing import List from fastapi import FastAPI from loguru import logger from ai_web_app.main import Result, index_embeddings, search app = FastAPI() logger.info(\u0026#34;Indexing database...\u0026#34;) database_path = Path(\u0026#34;./articles.sqlite\u0026#34;) embeddings = index_embeddings(database_path) @app.get(\u0026#34;/search\u0026#34;) async def search_endpoint(query: str, topn: int = 5) -\u0026gt; List[Result]: results = search(embeddings, database_path, query, topn=topn) return results To make this easier to run, we should also add a new script to our pyproject.toml file (add the following script to the appropriate section)\n[tool.hatch.envs.default.scripts] serve = \u0026#34;uvicorn app:app --port 8080\u0026#34; and then we can simply run this command to start the server:\nhatch run serve Running that command should index our database (took ~10s on my computer) and start serving requests on port 8080.\nWe can test how this looks like by running in another terminal window\ncurl -X GET \u0026#34;http://127.0.0.1:8080/search?query=symptoms%20of%20covid\u0026#34; If you’ve been following so far, this is what you should get as a response   Integration Tests To make sure our backend endpoint behaves as expected even if we change it in the future, we should set up some simple integration tests.\nFor this, we will use pytest-xprocess to launch our server as a separate process, and then make some simple requests and assert that the response is as expected.\nFirst of all, we need to add pytest-xprocess to our dev dependencies in pyproject.toml.\nAfter that, we create a file tests/test_app.py:\nimport pytest import requests from xprocess import ProcessStarter class TestPythonServer: @pytest.fixture(scope=\u0026#34;module\u0026#34;) def server(self, xprocess): class Starter(ProcessStarter): timeout = 600 pattern = \u0026#34;Application startup complete\u0026#34; args = [\u0026#34;hatch\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;serve\u0026#34;] xprocess.ensure(\u0026#34;server\u0026#34;, Starter) url = \u0026#34;http://127.0.0.1:8080\u0026#34; yield url xprocess.getinfo(\u0026#34;server\u0026#34;).terminate() @pytest.mark.integration def test_valid_search(self, server): topn = 5 response = requests.get( server + \u0026#34;/search\u0026#34;, params={\u0026#34;query\u0026#34;: \u0026#34;symptoms of covid\u0026#34;, \u0026#34;topn\u0026#34;: topn} ) assert response.status_code == 200 assert len(response.json()) == topn @pytest.mark.integration def test_empty_search(self, server): response = requests.get(server + \u0026#34;/search\u0026#34;) assert response.status_code == 422 This file defines a class with 3 methods:\n server starts up our web server, and waits for it to be available. It’s configured to wait until it sees the words \u0026#34;Application startup complete\u0026#34; in the output of hatch run serve. test_valid_search makes a simple request and assures that the response is valid, and has the correct number of requested results. You could spend a lot of time here coming up with better test cases, but for this simple app there’s not that much that can go wrong. test_empty_search tests a case where the incoming request doesn’t have any query. In this case, the server should return an error. It is important to verify your assumptions of the behavior for failing cases, so you don’t have silent errors in your system.  You can also add this as a new script in the pyproject.toml file:\n[tool.hatch.envs.default.scripts] integration = \u0026#34;no-cov -m integration -x\u0026#34; One final thing for the integration tests: typically these tests are quite slow to run, as they require an often lengthy setup process. As such, we shouldn’t run these tests every time.\nIn the file above, we marked the tests with @pytest.mark.integration. This allows us to configure pytest to not run those tests by default. We can do that by adding this to our pyproject.toml file:\n[tool.pytest.ini_options] addopts = \u0026#34;--strict-markers -m \\\u0026#34;not integration\\\u0026#34;\u0026#34; markers = [ \u0026#34;integration: slow tests, shouldn\u0026#39;t be run so often\u0026#34; ] Now we can differentiate between running …","date":1678233900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233900,"objectID":"4c2468056ca0ff054204c3725dd20d3b","permalink":"https://dcferreira.com/post/2023-03-05-ai-web-app/","publishdate":"2023-03-08T00:05:00Z","relpermalink":"/post/2023-03-05-ai-web-app/","section":"post","summary":"How to use FastAPI+Uvicorn to write and deploy the backend of a simple AI App.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 5)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.   In this post we’ll have a look test-driven development. The idea is to first write some simple unit tests, where we describe how our code should work. Doing this forces you to think about what functions you actually need to write and, if you’re part of a team, helps ensure that everyone is one the same page. Only after having a few simple tests will we actually start writing code, and make it pass the unit tests.\nFor a smaller project that you do not which to maintain for a long time, having these simple unit tests is not a strong necessity. But be warned: your project will eventually break if used enough, and unit tests will likely be your only warning.\nSetup In our example we have a database with articles about COVID-19 (see Part 2). For our unit tests, we want to use the same data, but a lot smaller so tests run fast, and we have control of the outputs.\nLet’s then think about what kind of simple examples we need. The database has a articles table and a sections table. Let’s open up the database and extract 2 articles, and a couple of sections from each of them. It’s important here that you choose examples that are helpful. In this case I chose 2 sections from each article: a Title and an Abstract, because the Titles won’t be indexed in our use case (see Part 2 of this series for details). I also chose sections that are different enough among themselves that I can make a query for each of them, and verify that the result is the section I wanted.\nPut these 2 files in tests/assets:\n  articles.csv:\nid,source,published,publication,authors,title,tags,design,size,sample,method,reference,entry 071hvhme,WHO,2020-01-01 00:00:00,MSMR,\u0026#34;Kebisek, Julianna; Forrest, Lanna J; Maule, Alexis L; Steelman, Ryan A; Ambrose, John F\u0026#34;,\u0026#34;Special report: Prevalence of selected underlying health conditions among active component Army service members with coronavirus disease 2019, 11 February-6 April 2020\u0026#34;,COVID-19,0,873,\u0026#34;COVID-19 has been a reportable condition for the Department of Defense since 5 February 2020, and, as of the morning of 6 April, a total of 873 cases were reported to the Disease Reporting System internet from Army installations.\u0026#34;,,https://doi.org/,2020-06-07 08as6hga,WHO,2020-01-01 00:00:00,Int. j. sports med,\u0026#34;Stokes, Keith A; Jones, Ben; Bennett, Mark; Close, Graeme L; Gill, Nicholas; Hull, James H; Kasper, Andreas M; Kemp, Simon P T; Mellalieu, Stephen D; Peirce, Nicholas; Stewart, Bob; Wall, Benjamin T; West, Stephen W; Cross, Matthew\u0026#34;,Returning to Play after Prolonged Training Restrictions in Professional Collision Sports,COVID-19,0,,The COVID-19 pandemic in 2020 has resulted in widespread training disruption in many sports.,,https://doi.org/,2020-06-08   sections.csv:\nid,article,tags,design,name,text,labels 1749694,071hvhme,COVID-19,0,TITLE,\u0026#34;Special report: Prevalence of selected underlying health conditions among active component Army service members with coronavirus disease 2019, 11 February-6 April 2020\u0026#34;,FRAGMENT 1749697,071hvhme,COVID-19,0,ABSTRACT,\u0026#34;COVID-19 has been a reportable condition for the Department of Defense since 5 February 2020, and, as of the morning of 6 April, a total of 873 cases were reported to the Disease Reporting System internet from Army installations.\u0026#34;,SAMPLE_SIZE 1867855,08as6hga,COVID-19,0,TITLE,Returning to Play after Prolonged Training Restrictions in Professional Collision Sports,FRAGMENT 1867856,08as6hga,COVID-19,0,ABSTRACT,The COVID-19 pandemic in 2020 has resulted in widespread training disruption in many sports.,SAMPLE_SIZE   We also need to setup some fixtures to access these files. The original data is a SQLite file with 2 tables, so we need to write a fixture that reads the CSV files and generates a SQLite file.\nCreate a tests/conftest.py file:\nimport sqlite3 from pathlib import Path from tempfile import NamedTemporaryFile import pandas as pd import pytest @pytest.fixture(scope=\u0026#34;module\u0026#34;) def assets_path(): return Path(__file__).resolve().parent / \u0026#34;assets\u0026#34; @pytest.fixture(scope=\u0026#34;module\u0026#34;) def articles_database(assets_path) -\u0026gt; Path: with NamedTemporaryFile() as f: conn = sqlite3.connect(f.name) # load CSVs articles = pd.read_csv(assets_path / \u0026#34;articles.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) sections = pd.read_csv(assets_path / \u0026#34;sections.csv\u0026#34;, sep=\u0026#34;,\u0026#34;) # write CSVs to DB articles.to_sql(\u0026#34;articles\u0026#34;, conn, index=False) sections.to_sql(\u0026#34;sections\u0026#34;, conn, index=False) yield Path(f.name) You can then use these fixtures for all your tests.\nWriting tests We can already make a skeleton of what we want, before actually starting programming. Let’s then make a ai_web_app/main.py file, where we will in the future implement the functions that we used in Part 2:\nfrom pathlib import Path from txtai.embeddings import Embeddings def index_embeddings(database: Path) -\u0026gt; Embeddings: raise NotImplementedError def search(embeddings: Embeddings, database: Path, query: str, topn: int = 5): raise …","date":1678233840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233840,"objectID":"f4293b93ac02246b1969d1c18c32d4ed","permalink":"https://dcferreira.com/post/2023-03-04-ai-web-app/","publishdate":"2023-03-08T00:04:00Z","relpermalink":"/post/2023-03-04-ai-web-app/","section":"post","summary":"How and why we should use pytest and unit tests to facilitate development when writing an AI App.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 4)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.   In this post we will have a look at how to setup Python projects using modern tools in the Python ecosystem. This post is all about the tools, and there’s basically no code.\nThe tools recommended here are mostly very new, and in my opinion, better than the older alternatives. If you haven’t yet tried these tools but are familiar with the alternatives, I’d suggest you give them a go: all of these tools had basically 0 friction while integrating with my workflows.\nProject Manager  Alternatives to hatch: Poetry, PDM, Conda, venv.   My currently favorite project manager is hatch. If you’ve never used a project manager before, its main job is keeping track of your dependencies, making sure there are no incompatibilities between libraries, and keeping your environment clean of unnecessary clutter. Project managers also allow you to build or publish your own Python packages with a single command.\nI like hatch in particular because it’s super fast at resolving dependencies (unlike Poetry [1] [2]) and has a utility for running scripts and an environment manager. These last 2 features allow hatch to replace tox for many use cases.\n At the moment hatch has a big downside compared to Poetry, in that it can’t pin dependencies. You can specify the library versions you want, but their dependencies’ versions won’t be fixed. (See their FAQ about libraries vs applications)\nHopefully this will be solved in the future.\n  So the first step is to install hatch\npip install hatch We can then start a new project by running the following command and giving a project name and description\nhatch new -i This command will create a new directory and populate it with an empty template\nai-web-app ├── ai_web_app │ ├── __about__.py │ └── __init__.py ├── tests │ └── __init__.py ├── LICENSE.txt ├── README.md └── pyproject.toml The most interesting file here is pyproject.toml, where you will keep all your project’s configurations.\nLinter  Alternatives to ruff: flake8, FlakeHeaven, FlakeHell.\nI’m not aware of good alternatives to black or isort.\n  For linting, my favorite tool is ruff. It’s significantly faster than any other linter, and very simple to setup and forget.\nAnother great tool for keeping the style the same throughout the code is black. It’s an opinionated code formatter. I love it because I can just set it up in my IDE to run whenever I save a file, and then I don’t have to worry about style or formatting when writing the code. If you’re collaborating on the same code with someone else, black is really indispensable: it forces your team to keep the same style throughout the code, and it makes the git differences much simpler.\nThe final tool here is isort. This is a very simple tool, which forces imports to have a specific order. That’s it! That also makes the git diffs simpler, and is simple to setup and forget.\nTo install all of these tools, you just need to edit the pyproject.toml file. Look for the dev dependencies section (it’s the default env in hatch), and just add them there. At this point, your dev dependencies section should look like this:\n[tool.hatch.envs.default] dependencies = [ \u0026#34;pytest\u0026#34;, \u0026#34;pytest-cov\u0026#34;, \u0026#34;black\u0026#34;, \u0026#34;isort\u0026#34;, \u0026#34;ruff\u0026#34;, ] isort needs a bit of configuration to play nice with black (see their compatibility guide). Just add the following to the end of pyproject.toml:\n[tool.isort] profile = \u0026#34;black\u0026#34; The final step in this section is to make use of hatch and its scripting features to run our linting. Look for the scripts section in the pyproject.toml file ([tool.hatch.envs.default.scripts]) and these 2 new lines (the first lines should be there if you used hatch init):\n[tool.hatch.envs.default.scripts] cov = \u0026#34;pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=ai_web_app --cov=tests {args}\u0026#34; no-cov = \u0026#34;cov --no-cov {args}\u0026#34; lint = [\u0026#34;ruff .\u0026#34;, \u0026#34;black . --check -q\u0026#34;, \u0026#34;isort . --check -q\u0026#34;] format = [\u0026#34;black .\u0026#34;, \u0026#34;isort .\u0026#34;] Pre-commit hooks Another essential tool, especially if you work in a team: pre-commit git hooks. These are scripts that will be executed before every commit you make, to make sure you don’t make a stupid mistake and commit it into the repo. You can enforce linting in this step, and make it automatic.\nFor this you need the pre-commit Python package (just add it to your dev environment, like above), and a config file. You can just take my configuration, which uses the tools mentioned above. Create a .pre-commit-config.yaml file and paste this:\nrepos:- repo:https://github.com/pre-commit/pre-commit-hooksrev:v2.3.0hooks:- id:check-yaml- id:end-of-file-fixer- id:trailing-whitespace- repo:https://github.com/pycqa/isortrev:5.11.2hooks:- id:isortname:isort (python)- repo:https://github.com/psf/blackrev:22.12.0hooks:- id:black- repo:https://github.com/charliermarsh/ruff-pre-commit# Ruff version.rev:\u0026#34;v0.0.211\u0026#34;hooks:- id:ruff# Respect `exclude` and `extend-exclude` …","date":1678233780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233780,"objectID":"14e47f2a2d9034e985b4e0d2601af43c","permalink":"https://dcferreira.com/post/2023-03-03-ai-web-app/","publishdate":"2023-03-08T00:03:00Z","relpermalink":"/post/2023-03-03-ai-web-app/","section":"post","summary":"How to setup a Python project for easy developing with black, ruff, pre-commit hooks, and type hints.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 3)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":" This is part of a multi-part blogpost about how to build an AI Web App. Please refer to Part 1 for more context.\nThis post uses txtai as the AI library.\nAlternatives would be: transformers, sentence-transformers, PyTorch, Keras, scikit-learn, among many others.\n  The first step in any project is always to make a proof of concept. At this stage, we don’t care about performance, edge cases, or any other intricacies— we just want to confirm that the project is viable.\nFor the sake of example, we will take strong inspiration from this example from the txtai library. We’ll index a database of documents and then query for them with natural language. Basically we’ll be implementing our own little Google Search, which only returns results from the files we feed it.\nBut we won’t just search for keywords, like classical search engines. It’s 2023 and we have AI at our disposal! We will use text embeddings to search for the closest match. By searching for embeddings we’re not literally searching for the words we give it, but for the meaning of the whole query. This is called Semantic search.\nSo, let’s get thing going by creating a new directory for our little project:\nmkdir ai-web-app cd ai-web-app We will need a database to index and query. For this example, we will use a test dataset that txtai made available. This data is a collection of research documents into COVID-19. The original version of this dataset can be found on Kaggle.\nWe download the dataset with the following shell commands:\nwget https://github.com/neuml/txtai/releases/download/v1.1.0/tests.gz gunzip tests.gz mv tests articles.sqlite and install the needed libraries with\npip install txtai sentence-transformers pandas jupyterlab We can then start a notebook server with Jupyter Lab with\nmkdir notebooks \u0026amp;\u0026amp; cd notebooks \u0026amp;\u0026amp; jupyter lab From our database, we will need to create an index (a searchable database of documents), and a way to query (search) the index. The following code (mostly stolen from txtai’s example) creates an index with the COVID-19 dataset downloaded above:\nimport sqlite3 import regex as re from txtai.embeddings import Embeddings from txtai.pipeline import Tokenizer def stream(): # Connection to database file db = sqlite3.connect(\u0026#34;../articles.sqlite\u0026#34;) cur = db.cursor() # Select tagged sentences without a NLP label. NLP labels are set for non-informative sentences. cur.execute(\u0026#34;SELECT Id, Name, Text FROM sections WHERE (labels is null or labels NOT IN (\u0026#39;FRAGMENT\u0026#39;, \u0026#39;QUESTION\u0026#39;)) AND tags is not null\u0026#34;) count = 0 for row in cur: # Unpack row uid, name, text = row # Only process certain document sections if not name or not re.search(r\u0026#34;background|(?\u0026lt;!.*?results.*?)discussion|introduction|reference\u0026#34;, name.lower()): # Tokenize text tokens = Tokenizer.tokenize(text) document = (uid, tokens, None) count += 1 if count % 1000 == 0: print(\u0026#34;Streamed %ddocuments\u0026#34; % (count), end=\u0026#34;\\r\u0026#34;) # Skip documents with no tokens parsed if tokens: yield document print(\u0026#34;Iterated over %dtotal rows\u0026#34; % (count)) # Free database resources db.close() # BM25 + fastText vectors embeddings = Embeddings({ \u0026#34;method\u0026#34;: \u0026#34;sentence-transformers\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;all-MiniLM-L6-v2\u0026#34;, \u0026#34;scoring\u0026#34;: \u0026#34;bm25\u0026#34; }) embeddings.score(stream()) embeddings.index(stream()) In the code above, the text embeddings are generated using the all-MiniLM-L6-v2 model. We could also train our own model instead of using a pre-trained model, but that would require a lot of work. If we can avoid it, why not?\nSo at this point, we have a database and want to search it. In the following cell, we define a Python function to query the index for the closest results, and then query the original data for extra information about the results. The last thing we do in this cell is to search for \u0026#34;risk factors\u0026#34;.\nimport pandas as pd pd.set_option(\u0026#34;display.max_colwidth\u0026#34;, None) def search(query: str, topn: int = 5) -\u0026gt; pd.DataFrame: db = sqlite3.connect(\u0026#34;../articles.sqlite\u0026#34;) cur = db.cursor() results = [] for uid, score in embeddings.search(query, topn): cur.execute(\u0026#34;SELECT article, text FROM sections WHERE id = ?\u0026#34;, [uid]) uid, text = cur.fetchone() cur.execute(\u0026#34;SELECT Title, Published, Reference from articles where id = ?\u0026#34;, [uid]) results.append(cur.fetchone() + (text,)) db.close() df = pd.DataFrame(results, columns=[\u0026#34;Title\u0026#34;, \u0026#34;Published\u0026#34;, \u0026#34;Reference\u0026#34;, \u0026#34;Match\u0026#34;]) return df search(\u0026#34;risk factors\u0026#34;) The output of that cell will be a pandas dataframe with the results:   We can also try other queries:     As you can see, while the results maybe aren’t the best, they are good enough for our proof of concept. We could spend a lot more time making this part better, but in the spirit of shipping early and often, this is enough for now.\n If you’ve been following these instructions, your code should look like this: https://github.com/dcferreira/ai-web-app/tree/19cda18ea099d46716b280694eed2677ef680e5d   To continue this tutorial, go to Part 3.\n","date":1678233720,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233720,"objectID":"e27ea20c3ee01b2c0aefbc2c84faadbb","permalink":"https://dcferreira.com/post/2023-03-02-ai-web-app/","publishdate":"2023-03-08T00:02:00Z","relpermalink":"/post/2023-03-02-ai-web-app/","section":"post","summary":"How and why to make a proof of concept for an AI app, using Jupyter Notebook.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 2)","type":"post"},{"authors":["Daniel C. Ferreira"],"categories":["AI","NLP"],"content":"Did you hear the news? The days of training your own Machine Learning models are over! In 2023, the best way to make an AI Web App is to not train any model at all. After all, “the hottest new programming language is English”. Now it’s easier and cheaper than ever to make an AI Web App.\nThanks to communities such HuggingFace models and new popular APIs (such as OpenAI, co:here, or AssemblyAI), we now have access to lots of high-quality pre-trained models.\nIn this blog post series, we will go over how to develop an AI Web App using modern Python tools, and deploy it almost for free on the cloud using serverless infrastructure. We will take an example of a simple AI search engine in Python, and take it all the way from concept to deployment. This tutorial will hold your hand through the whole process with lots of code and commands, and give you resources to look at when your use case differs.\nFor this guide, we’re assuming you’re using Linux (or WSL on Windows), and are comfortable with command line and Python. For the deployment part we also assume you have a valid credit card, but that part is optional and some alternatives are suggested.\nHere is the full table of contents:\n Introduction (this page) Make a Proof of Concept for an AI App Setup the Python Project with Modern Tools Develop an App with Test-Driven Development Implement and Test a Python Backend Server Containerize an App with Docker Build a CI/CD Pipeline for an AI App Deploy a Serverless AI App with Google Cloud Deploy a Simple Frontend for an AI App  See you in Part 2!\n","date":1678233660,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678233660,"objectID":"102a5c181951ebd68fe047df0c28e537","permalink":"https://dcferreira.com/post/2023-03-01-ai-web-app/","publishdate":"2023-03-08T00:01:00Z","relpermalink":"/post/2023-03-01-ai-web-app/","section":"post","summary":"A complete guide to developing and deploying an AI Web App with modern tools.","tags":["AI","Machine Learning","Web App","txtai","Web Development","Serverless","NLP"],"title":"Making and Deploying an AI Web App in 2023 (Part 1)","type":"post"},{"authors":null,"categories":null,"content":"This was a short project I did with a random team for a weekend-long Hackathon. The idea was to take the most liked tweets for a specific user, and use a Large Language Model to generate new tweets in this user’s style.\nAt the moment you can try it out at https://tweetfake.dcferreira.com (need to be logged in to Twitter), although it’s not clear for how long this link will stay up. There is also a short video in the devpost project page.\n","date":1670716800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670716800,"objectID":"78edb8babe78801a704cf73897c1bb69","permalink":"https://dcferreira.com/project/tweetfake/","publishdate":"2022-12-11T00:00:00Z","relpermalink":"/project/tweetfake/","section":"project","summary":"This was a short project I did with a random team for a weekend-long Hackathon. The idea was to take the most liked tweets for a specific user, and use a Large Language Model to generate new tweets in this user’s style.","tags":["NLP"],"title":"Tweet Fake","type":"project"},{"authors":null,"categories":null,"content":"A simple browser extension that hides YouTube video length for user-selected channels. Unlike other similar browser extensions, this one also hides the video length in video thumbnails.\nYou can easily install it from the Chrome Web Store.\n","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"cd93aba32aec085e8a42dcfce593d476","permalink":"https://dcferreira.com/project/infinity-for-youtube/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/project/infinity-for-youtube/","section":"project","summary":"A simple browser extension that hides YouTube video length for user-selected channels. Unlike other similar browser extensions, this one also hides the video length in video thumbnails.\nYou can easily install it from the Chrome Web Store.","tags":["Webdev"],"title":"Infinity for Youtube","type":"project"},{"authors":["Daniel C. Ferreira"],"categories":["Spark","Databricks"],"content":"Overview I often run into a problem when writing UDFs on Databricks, where I need some to access some object that pickle can’t serialize. Often times this is just something that comes from some external library, and so fixing the code is not a practical solution.\nAn easy solution to this is to initialize the object inside the UDF itself. This avoids the need for serialization, but it introduces a new problem: the object is initialized for every run of the UDF, hitting performance.\nThe solution that addresses these 2 problems is to cache the object initialization. Then, each executor initializes the object only once.\nThe Problem Here is a simple example:\nimport time from lxml.etree import HTMLParser # `spark` is the spark context, on databricks it is a global variable that\u0026#39;s always available df = spark.createDataFrame([{\u0026#34;n\u0026#34;: n} for n in range(10000)]) class Slow: def __init__(self): self.parser = HTMLParser() time.sleep(0.01) def double(self, x: int) -\u0026gt; int: return 2 * x slow_global = Slow() @udf(\u0026#34;int\u0026#34;) def f_error(n): return slow_global.double(n) When actually executing the UDF\ndf.select(\u0026#34;n\u0026#34;, f_error(\u0026#34;n\u0026#34;)).collect() we get the error\nPicklingError: Could not serialize object: TypeError: can\u0026#39;t pickle lxml.etree.HTMLParser objects Naive Solution The naive solution is to initialize the object in each run of the UDF:\n@udf(\u0026#34;int\u0026#34;) def f(n): slow = Slow() return slow.double(n) This works\ndf.select(\u0026#34;n\u0026#34;, f(\u0026#34;n\u0026#34;)).collect() but it’s very inefficient.\nOn a cluster with 2 i3.xlarge workers on AWS, executing this took me around 25 seconds.\nOptimized Solution The solution is then to cache the object initialization. For this, we need the cachetools library. On Databricks, you can install it by running the following cell\n%pip install cachetools  We can’t use lru_cache from the standard library, because it requires serialization. Trying it gives us the error: PicklingError: Could not serialize object: AttributeError: \u0026#39;functools._lru_cache_wrapper\u0026#39; object has no attribute \u0026#39;__bases__\u0026#39;   Usage is very simple:\nfrom cachetools import cached @cached(cache={}) def get_slow(): return Slow() @udf(\u0026#34;int\u0026#34;) def f_cached(n): slow = get_slow() return slow.double(n) Executing it\ndf.select(\u0026#34;n\u0026#34;, f_cached(\u0026#34;n\u0026#34;)).collect() took around 0.5 seconds, in the same cluster as above.\n","date":1647883800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647883800,"objectID":"70ebad5652ed1089e6f2d52e9dc93813","permalink":"https://dcferreira.com/post/2022-03-spark-serialization/","publishdate":"2022-03-21T17:30:00Z","relpermalink":"/post/2022-03-spark-serialization/","section":"post","summary":"How to avoid `PicklingError` on custom UDFs on Databricks/Spark, while keeping optimal performance.","tags":["Spark","Databricks"],"title":"Efficient UDFs on Databricks with unpickleable objects","type":"post"},{"authors":["Félix Iglesias Vázquez","Daniel C. Ferreira","Gernot Vormayr","Maximilian Bachl","Tanja Zseby"],"categories":null,"content":"","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580342400,"objectID":"e533a110dfaa3ca4a458371794ddb620","permalink":"https://dcferreira.com/publication/ntarc/","publishdate":"2020-10-30T00:00:00Z","relpermalink":"/publication/ntarc/","section":"publication","summary":"We present Network Traffic Analysis Research Curation (NTARC), a data model to store key information about network traffic analysis research.","tags":[],"title":"NTARC: A Data Model for the Systematic Review of Network Traffic Analysis Research","type":"publication"},{"authors":null,"categories":null,"content":"","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572393600,"objectID":"e177ab8b66e73093bc342f05b68834fd","permalink":"https://dcferreira.com/project/deep-architect/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/project/deep-architect/","section":"project","summary":"DeepArchitect is a framework for automatically searching over computational graphs in arbitrary domains, designed with a focus on modularity, ease of use, reusability, and extensibility.","tags":["Deep Learning"],"title":"DeepArchitect","type":"project"},{"authors":["Renato Negrinho","Matthew Gormley","Geoffrey J Gordon","Darshan Patil","Nghia Le","Daniel C. Ferreira"],"categories":null,"content":"","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572393600,"objectID":"07bec4b4eeec94caf472068b9a47405a","permalink":"https://dcferreira.com/publication/deep-architect/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/deep-architect/","section":"publication","summary":"We propose a formal language for encoding search spaces over general computational graphs, applicable in particular to neural network architecture search.","tags":[],"title":"Towards modular and programmable architecture search","type":"publication"},{"authors":["Félix Iglesias Vázquez","Tanja Zseby","Daniel C. Ferreira","Arthur Zimek"],"categories":null,"content":"","date":1569888e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888e3,"objectID":"92be01b4455e94c39bae18def642b089","permalink":"https://dcferreira.com/publication/mdcgen/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/mdcgen/","section":"publication","summary":"We present a tool for generating multidimensional synthetic datasets for testing, evaluating, and benchmarking unsupervised classification algorithms.","tags":[],"title":"MDCGen: Multidimensional dataset generator for clustering","type":"publication"},{"authors":["Maximilian Bachl","Daniel C. Ferreira"],"categories":null,"content":"","date":1562112e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562112e3,"objectID":"589124e1206e40ba704b1d84403ab9b1","permalink":"https://dcferreira.com/publication/city-gan/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/city-gan/","section":"publication","summary":"We use GANs to generate images of buildings' facades, stylized according to a chosen city.","tags":[],"title":"City-GAN: Learning architectural styles using a custom Conditional GAN architecture","type":"publication"},{"authors":null,"categories":null,"content":"Generative Adversarial Networks (GANs) are a well-known technique that is trained on samples (e.g. pictures of fruits) and which after training is able to generate realistic new samples. Conditional GANs (CGANs) additionally provide label information for subclasses (e.g. apple, orange, pear) which enables the GAN to learn more easily and increase the quality of its output samples. We use GANs to learn architectural features of major cities and to generate images of buildings which do not exist. We show that currently available GAN and CGAN architectures are unsuited for this task and propose a custom architecture and demonstrate that our architecture has superior performance for this task and verify its capabilities with extensive experiments.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"482a39f7418c7ee7470038a7fa3a498d","permalink":"https://dcferreira.com/project/city-gan/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/project/city-gan/","section":"project","summary":"City-GAN uses Conditional Generative Adversarial Neural Networks to generate pictures of fake buildings, with the architectural characteristics of a specific city.","tags":["Deep Learning"],"title":"City-GAN","type":"project"},{"authors":["Daniel C. Ferreira","Félix Iglesias Vázquez","Tanja Zseby"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"a4aac82b15bee1f5d798cbfe3d6bb1b7","permalink":"https://dcferreira.com/publication/feature-reduction/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/feature-reduction/","section":"publication","summary":"We used semi-supervised Autoencoders to obtain 2d visualizations of network traffic that separate between distinct types of attacks.","tags":[],"title":"Extreme Dimensionality Reduction for Network Attack Visualization with Autoencoders","type":"publication"},{"authors":null,"categories":null,"content":"","date":1551916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551916800,"objectID":"f3e2e8505a3f890c5ffe9840d37e8bb0","permalink":"https://dcferreira.com/project/tfm/","publishdate":"2019-03-07T00:00:00Z","relpermalink":"/project/tfm/","section":"project","summary":"The Traffic Flow Mapper is a prototype tool for visualizing network traffic in 2D.","tags":["Security"],"title":"Traffic Flow Mapper","type":"project"},{"authors":null,"categories":null,"content":"","date":1515283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515283200,"objectID":"0509e52eaaf4b3b9a8948cd7fb7952f5","permalink":"https://dcferreira.com/project/mdcgenpy/","publishdate":"2018-01-07T00:00:00Z","relpermalink":"/project/mdcgenpy/","section":"project","summary":"MDCGenPy is a synthetic dataset generator made specifically for testing clustering algorithms. It allows for incredible flexibility in generating data with specific shapes with a low effort.","tags":["Clustering"],"title":"MDCGenPy","type":"project"},{"authors":["Daniel C. Ferreira","Félix Iglesias Vázquez","Gernot Vormayr","Maximilian Bachl","Tanja Zseby"],"categories":null,"content":"","date":1502064e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502064e3,"objectID":"3e8b2820ea4e05a56de1769aa014ba2b","permalink":"https://dcferreira.com/publication/ntarc-features/","publishdate":"2020-10-30T00:00:00Z","relpermalink":"/publication/ntarc-features/","section":"publication","summary":"We analyse the used features in network traffic research, and propose a new traffic vector based on how often they are chosen in the literature.","tags":[],"title":"A meta-analysis approach for feature selection in network traffic research","type":"publication"},{"authors":null,"categories":null,"content":"","date":1502064e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502064e3,"objectID":"daae867e75600ed510fe2c812791d08c","permalink":"https://dcferreira.com/project/ntarc/","publishdate":"2017-08-07T00:00:00Z","relpermalink":"/project/ntarc/","section":"project","summary":"The NTARC database is a collective effort of labeling and categorizing research made in the network traffic analysis field.","tags":["Security"],"title":"NTARC Database","type":"project"},{"authors":["Daniel C. Ferreira","André FT Martins","Mariana SC Almeida"],"categories":null,"content":"","date":1470528e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470528e3,"objectID":"27ce786a283d0a51b69801ddc9675806","permalink":"https://dcferreira.com/publication/multilingual/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/publication/multilingual/","section":"publication","summary":"We propose a joint formulation for learning task-specific cross-lingual word embeddings, along with classifiers for that task. We obtain state of the art results in multiple multilingual datasets.","tags":[],"title":"Jointly Learning to Embed and Predict with Multiple Languages","type":"publication"},{"authors":null,"categories":null,"content":"","date":1470528e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470528e3,"objectID":"5554c872e2ca1ae2a3e10a6be29b0ba7","permalink":"https://dcferreira.com/project/multilingual-embeddings/","publishdate":"2016-08-07T00:00:00Z","relpermalink":"/project/multilingual-embeddings/","section":"project","summary":"Multilingual embeddings can be used for any Natural Language Processing task which applies to multiple languages.","tags":["Machine Learning","NLP"],"title":"Multilingual Embeddings","type":"project"}]